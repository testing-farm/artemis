---
title: "Drivers"
date: 2021-08-11T08:10:36+01:00
draft: false
---
:sectlinks:
:showtitle:

== Drivers

A driver acts on the boundary between Artemis core and a given provisioning service. By implementing basic methods like "inspect VM" or "release resources", provides necessary level of polymorphism, allowing Artemis to switch transparently between pools as needed.

=== Driver capabilities

Capabilities are features whose support is built-in into the driver. The support is usually optional, and the feature may be disabled, but it is not possible to enable feature driver does not support.

* `supports-snapshots`: the driver can handle snapshots of some kind.
* `supports-spot-instances`: the driver can handle spot instance requests.
* `supports-native-post-install-script`: the driver can handle the post-installation script on its own. Artemis core will execute the script in the preparation stage for drivers that do no have this capability.

[%header,cols="1,1,1,1"]
|===
|Driver
|supports-snapshots
|supports-spot-instances
|supports-native-post-install-script

|`aws`
|[red]#no#
|[green]#yes#
|[green]#yes#

|`azure`
|[red]#no#
|[red]#no#
|[green]#yes#

|`beaker`
|[red]#no#
|[red]#no#
|[red]#no#

|`localhost`
|[red]#no#
|[red]#no#
|[red]#no#

|`openstack`
|[green]#yes#
|[red]#no#
|[green]#yes#

|===


=== HW requirements

A guest request can specify various HW constraints the provisioned machines must satisfy. For example, a desired number of CPU cores or a minimal root disk size. These constraints are eventually used by drivers to find - or create - suitable guests. Unfortunately, not all drivers are capable of handling all possible HW requirements, limitations may apply.

[NOTE]
====
The table below describe the built-in support for various features. However, to actually support a given feature, a maintainer may need to provide additional configuration - see <<_flavors_and_images,"Flavors and images">> section. Such requirements are marked with an asterisk (`*`).
====

[%header,cols="1,1,1,1,1,1"]
|===
|HW constraint
|`aws`
|`azure`
|`beaker`
|`localhost`
|`openstack`

|`arch`
|[green]#yes#
|[green]#yes#
|[green]#yes#
|[green]#yes#
|[green]#yes#

|`boot.method`
|[red]#no#
|[red]#no#
|[green]#yes# *
|[red]#no#
|[red]#no#

|`cpu.cores`
|[green]#yes#
|[red]#no#
|[green]#yes#
|[red]#no#
|[green]#yes#

|`cpu.family`
|[green]#yes# *
|[red]#no#
|[red]#yes#
|[red]#no#
|[green]#yes# *

|`cpu.family_name`
|[green]#yes# *
|[red]#no#
|[red]#no#
|[red]#no#
|[green]#yes# *

|`cpu.model`
|[green]#yes# *
|[red]#no#
|[green]#yes# *
|[red]#no#
|[green]#yes# *

|`cpu.model_name`
|[green]#yes# *
|[red]#no#
|[green]#yes# *
|[red]#no#
|[green]#yes# *

|`cpu.processors`
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#

|`device.driver`
|[red]#no#
|[red]#no#
|[green]#yes#
|[red]#no#
|[red]#no#

|`disk[].space`
|[yellow]#partial# See <<hw-notes-only-one-disk, #1>>. *
|[red]#no#
|[yellow]#partial# See <<hw-notes-only-one-disk, #1>>.
|[red]#no#
|[yellow]#partial# See <<hw-notes-only-one-disk, #1>>.

|`hostname`
|[red]#no#
|[red]#no#
|[green]#yes#
|[red]#no#
|[red]#no#

|`memory`
|[green]#yes#
|[red]#no#
|[green]#yes#
|[red]#no#
|[green]#yes#

|`network[].type`
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#

|`system.model_name`
|[red]#no#
|[red]#no#
|[green]#yes#
|[red]#no#
|[red]#no#

|`virtualization.hypervisor`
|[green]#yes#
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#

|`virtualization.is_supported`
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#no#

|`virtualization.is_virtualized`
|[green]#yes#
|[red]#no#
|[red]#no#
|[red]#no#
|[red]#yes#

|===

1. [[hw-notes-only-one-disk]]`disk[]` supports only 1 item.

=== Flavors and images

Supported by: `aws` [line-through]#`azure`# [line-through]#`beaker`# [line-through]#`localhost`# `openstack`

Term "flavor" represents one half of a template for a future guest. Flavors track various attributes that affect the virtual "hardware" of the final machine backing the guest. For example, number of processors, RAM size, or CPU family.

Term "image" represents the second half of a template for a future guest. Images holds the content of the future virtual machine, file system with installed software, kernel, configuration and so on. There are also less visible attributes tracked by pools, e.g. whether an image supports UEFI or not.

Together, flavors and images play crucial role in provisioning process, because the set of flavors and and the set of images represent various guest configurations a pool can deliver, and based on this information pools allocate actual cloud resources for a given request. It is up to maintainers to setup pools and pools' flavors and images to provide the tiers of service most suited for their workflow.

[NOTE]
====
While all of the above is true, there are some pool drivers that do not rely on the concept of flavors or do not track available images. The most prominent driver would be `beaker` driver.

Beaker as a service approaches problem from a different angle, and instead of prepared set of flavors, Beaker comes with powerful filtering capabilities. It would be extremely impractical to collect all known "flavors", such a mapping between flavor and a Beaker machine would be almost identical, on top of that machines can be already in use which is not tracked by flavors at all.

However, this does not mean `beaker` driver is not able to pick proper machine for a given request or handle HW requirements: rather than collecting flavors and images, `beaker` driver transcripts the given guest request to Beaker's filter XML language, leaving the actual selection process to Beaker. It is still possible to create custom flavors and images in `beaker` configuration, but at its current state, such a configuration is ignored by the driver.
====

==== Collection of information

Pool drivers that work with flavors and images must keep track of known objects and their properties, This data must be kept up-to-date and reflect any kind of changes made by provisioning services backing their respective pools. For that purpose, drivers query their backend's APIs periodically, to download the current state of objects available to them. This process is automated, controlled by Artemis core, and the gathered information is cached.

[NOTE]
====
The automated, periodical refresh has its own pros and cons of which the following should be maintainers be aware of.

* given the caching, images and flavors information can be lacking slightly behind the changes in their backend, e.g. when new image is added. This delay should not exceed more than a few minutes, and changes should be picked up next time refresh occurs.
* after a cold start, Artemis' cache is probably going to be empty. This will affect any guest request submitted before the first refresh, pool driver would see no images to use for provisioning. But, thanks to inbuilt retries, cache refresh should happen soon enough for the guest request to proceed successfully after a delay.
+
To shorten the delay, maintainers can trigger an immediate refresh, see <<_runtime_overview,"Runtime overview">> section.
====

[NOTE]
====
Despite having full access to their backends, some information may be unavailable to drivers, and therefore some properties of images and flavors may remain unknown to them. For example, AWS does list what CPU models are used for each instance type, but this information is not accessible via AWS API.

In general, this lack of information is not important as long as requests don't depend on the missing information. Given the AWS example above: as long as requests Artemis is supposed to handle do not request particular CPU model, things will work out nicely.

Similar situation applies to images as well. For example, it's not possible for Artemis to extract username to use when connecting to guests via SSH. As long as image configuration matches the default username Artemis is using, `root`, then, again, things will work out nicely.

To handle more intricate requests and pool setup, maintainers might need to update configuration, see "<<_image_info_tweaks,Image info tweaks>>" and "<<_flavor_info_tweaks,Flavor info tweaks>>" sections.
====

==== Flavor info tweaks

Information pool tracks for all available flavors can be modified through configuration, using the `patch-flavors` and `custom-flavors` directives. Each _patch_ is applied to flavor or flavors matching given name (or regular expression), and overrides whatever the pool driver was able to collect from sources available to it in runtime.

Both directives share the same syntax, but their scope is slightly different:

* `custom-flavors` *adds new* flavors that do not exist as far as pool knows. For example, OpenStack driver can fetch list of existing flavors, `custom-flavors` then allows maintainer to create additional flavors on top of this basic list.
* `patch-flavors` *modifies existing* information known to pool, and does apply to flavors both real and created by `custom-flavors` directive.

[NOTE]
====
Entries under `patch-flavors` and `custom-flavors` are applied in order, it is therefore possible to modify multiple flavors at once, with `name-regex`, then tweak individual images using `name` for precise targeting.

Entries under `custom-flavors` are processed before `patch-flavors`, it is therefore possible to add flavors and then modify them. This might not seem like an advantage, but consider adding custom flavors while listing only the attributes they do not share, e.g. `cpu.family`. With `patch-flavors` applied later, it is possible to set attributes they share, e.g. `disk[0].size`, as long as their names can be matched by a regular expression in `name-regex`.
====

[NOTE]
====
All fields except `name`, `name-regex`, and `base` are optional, and fields not specified do not affect the value known to pool. It is therefore possible to change just a single attribute (e.g. `cpu.family`) without changing others (like `cpu.cores`).
====

.Specification
[source,yaml]
....
custom-flavors:
  - name: <string>
    # Name of already existing flavor that would serve as a template.
    # The flavor MUST exist, but it can be a custom flavor created before this patch.
    base: <string>

   cpu:
     processors: <integer>
     cores: <integer>
     family: <family>
     family_name: <string>
     model: <integer>
     model_name: <string>
     flag:
       - <string>
       ...

   disk:
     - size: <quantity>
       model-name: <string>

     # Or, to signal flavor can allocate additional disks
     - additional-disks:
         max-count: <integer>
         min-size: <quantity>
         max-size: <quantity>
         model-name: <string>

     ...

   virtualization:
     is-supported: <boolean>
     is-virtualized: <boolean>
     hypervisor: <string>
....

.Specification
[source,yaml]
....
custom-flavors:
  - name: <string>
    # Or, to patch multiple flavors at once:
    name-regex: <pattern>

   cpu:
     processors: <integer>
     cores: <integer>
     family: <family>
     family_name: <string>
     model: <integer>
     model_name: <string>
     flag:
       - <string>
       ...

   disk:
     - size: <quantity>
       model-name: <string>

     # Or, to signal flavor can allocate additional disks
     - additional-disks:
         max-count: <integer>
         min-size: <quantity>
         max-size: <quantity>

     ...

   virtualization:
     is-supported: <boolean>
     is-virtualized: <boolean>
     hypervisor: <string>
....

.Example
[source,yaml]
....
custom-flavors:
  # Let's add two custom flavors, with specific disk sizes. Both are based
  # on the same flavor, t2.small, and inherit all its properties.
  #
  # Also, all these flavors can get additional disks with actual size depending on a request.
  - name: t2.small-20
    base: t2.small
    disk:
      - size: 20 GiB
        model-name: PERC H310
      - additional-disks:
          max-count: 5
          min-size: 10 GiB
          max-size: 1 TiB

  - name: t2.small-40
    base: t2.small
    disk:
      - size: 40 GiB
      - additional-disks:
          model-name: PERC H310
          max-count: 5
          min-size: 10 GiB
          max-size: 1 TiB

patch-flavors:
  # Now, patch all flavors, and set fields we can't extract from pool's backend API.
  - name-regex: "t2\.small-\d+"
    cpu:
        family: 6
        family_name: Haswell
        model: 6
        model_name: i7-something
        flag:
          - fpu
          - vme
          - de
          ...

    # Oh, yes, all these flavors are VMs, not bare metal machines, and we support nested virtualization.
    virtualization:
        is-supported: true
        is-virtualized: true
        hypervisor: kvm

  # While technically possible, let's not use our smallest flavor for nested virtualization - not enough disk space.
  - name: t2.small-20
    virtualization:
        is-supported: false
....

==== Image info tweaks

Information pool tracks for all available images can be modified through configuration, using the `patch-images` directive. Each _patch_ is applied to image or images matching given name (or regular expression), and overrides whatever the pool driver was able to collect from sources available to it in runtime.

[NOTE]
====
Entries under `patch-images` are applied in order, it is therefore possible to modify multiple images at once, with `name-regex`, then tweak individual images using `name` for precise targeting.
====

[NOTE]
====
All fields except `name` and `name-regex` are optional, and fields not specified do not affect the value known to pool. It is therefore possible to change just a single attribute (e.g. `ssh.username`) without changing others (like `ssh.port`).
====

.Specification
[source,yaml]
....
patch-images:
  - name: <string>
    # Or, to patch multiple images at once:
    name-regex: <pattern>

    ssh:
      # Username to use when accessing guest based on this image via SSH
      username: <string>

      # Username to use when accessing guest based on this image via SSH
      port: <integer>
....

.Example
[source,yaml]
....
patch-images:
  # Reset the playing field: all images run SSH on port 22, and use `root` to log in.
  - name-regex: ".*"
    ssh:
      username: root
      port: 22

  # For Fedora ones, we need different username.
  - name-regex: "Fedora-.+"
    ssh:
      username: cloud-user

  # And one single image is just weird and runs its SSH on a high port.
  - name: Fedora-35
    ssh:
      port: 2222
....

==== Runtime overview

The most up-to-date information on known flavors and images can be displayed by querying API:

[source,shell]
....
$ http https://$hostname/_cache/pools/$poolname/image-info
$ http https://$hostname/_cache/pools/$poolname/flavor-info
....

It is also possible to trigger refresh of stored data with `POST` method, with no data:

[source,shell]
....
$ http POST https://$hostname/_cache/pools/$poolname/image-info
$ http POST https://$hostname/_cache/pools/$poolname/flavor-info
....

=== Guest logs

Supported by: `aws` `azure` `beaker` [line-through]#`gcp`# `ibmcloud-power` [line-through]#`ibmcloud-vpc`# [line-through]#`localhost`# `openstack`

Besides the operational logs related to guest provisioning, drivers often expose additional logs, usually related to the provisioning service actions or guest VM operations (terminal or console, output of `dmesg`, etc.).

The actual list of logs supported by a pool depends on the driver - this is a hard limit, logs that driver does not support cannot be "enabled" - and pool configuration, where maintainers can disable particular logs on purpose.

[%header,cols="1,1"]
|===
|Driver
|Supported logs

|`aws`
|console/blob
console/URL

|`azure`
|console/blob

|`beaker`
|console:dump/blob
console:dump/URL
sys.log:dump/URL
anaconda.log:dump/blob
storage.log:dump/blob
program.log:dump/blob
packaging.log:dump/blob
ks.cfg:dump/blob

|`gcp`
|-

|`ibmcloud-power`
|console/URL

|`ibmcloud-vpc`
|-

|`localhost`
|-

|`openstack`
|console/blob
console/URL

|===

==== Console support details

Console support varies significantly across cloud providers. This section provides detailed information about what console features are available for each driver.

[%header,cols="1,1,1,2"]
|===
|Driver
|Interactive console (URL)
|Console log (blob)
|Notes

|`aws`
|[green]#yes#
|[green]#yes#
|Interactive console uses AWS EC2 Instance Connect. Only available on instances with ENA (Elastic Network Adapter) support. Console URL is generated dynamically via a configurable template. See <<console-aws-notes,AWS notes>>.

|`azure`
|[red]#no#
|[yellow]#partial#
|Azure provides limited console log (blob) access. No interactive console URL is available.

|`beaker`
|[yellow]#external#
|[green]#yes#
|Console log available as blob or URL (pointing to Beaker's stored console output). System log (`sys.log`) also available. Interactive console available via conserver (external to Artemis).

|`gcp`
|[red]#no#
|[red]#no#
|GCP driver does not support console access.

|`ibmcloud-power`
|[green]#yes#
|[red]#no#
|Interactive console URL available via IBM Power Virtual Server console. No console log blob. URL expires after a configurable time (default: 300 seconds).

|`ibmcloud-vpc`
|[red]#no#
|[red]#no#
|IBM Cloud VPC driver does not currently support console access.

|`localhost`
|[red]#no#
|[red]#no#
|Localhost driver does not support console access.

|`openstack`
|[green]#yes#
|[green]#yes#
|Full console support. Interactive console URL (VNC/SPICE, depending on OpenStack configuration) via `openstack console url show`. Console log blob via `openstack console log show`. URL expires after a configurable time (default: 600 seconds).

|===

[[console-aws-notes]]
===== AWS console notes

AWS console URL support requires:

* The instance flavor must have ENA (Elastic Network Adapter) support. Flavors without ENA support are automatically filtered out when a guest requests `console:interactive` with URL content type.
* The console URL template is configurable via the `aws.logs.console.interactive.url` knob (environment variable: `ARTEMIS_AWS_LOGS_CONSOLE_INTERACTIVE_URL`).
* Default URL template: `https://console.aws.amazon.com/ec2/v2/connect/ec2-user/{instance_id}?connection-type=isc&serial-port=0`

[NOTE]
====
Due to the nature of AWS EC2 Instance Connect, there may be a slight lag when accessing the console compared to dedicated infrastructure like OpenStack or Beaker.
====

===== OpenStack console notes

OpenStack console support includes:

* Console URL is obtained via `openstack console url show` command.
* The type of console (VNC, SPICE, etc.) depends on the OpenStack deployment configuration.
* Console URL expiration is configurable via `openstack.console.url.expires` knob (environment variable: `ARTEMIS_OPENSTACK_CONSOLE_URL_EXPIRES`). Default: 600 seconds.

===== Beaker console notes

Beaker console and log support:

* Console log dumps are available as both URL (pointing to Beaker's stored output) and blob (content fetched from Beaker).
* System log (`sys.log:dump`) is available as URL.
* Installation logs are available as blobs: `anaconda.log:dump`, `storage.log:dump`, `program.log:dump`, `packaging.log:dump`.
* Kickstart configuration (`ks.cfg:dump`) is available as blob.
* Interactive console is available via conserver, which is external to Artemis. Users can connect directly to conserver using their Beaker credentials and machine hostname.

===== IBM Cloud Power console notes

IBM Cloud Power console support:

* Console URL is obtained via IBM Power Virtual Server API.
* URL expiration is configurable via `ibmcloud-power.console.url.expires` knob (environment variable: `ARTEMIS_IBMCLOUD_POWER_CONSOLE_URL_EXPIRES`). Default: 300 seconds.

[NOTE]
====
Disabling unsupported logs has no effect.
====

==== Gust log tweaks

Each pool can tune down the supported set of guest logs: while it is not possible to enable logs that are not already supported by pool's driver, it is still possible to disable supported logs, preventing users from accessing them.

.Specification
[source,yaml]
....
capabilities:
  disable-guest-logs:
    - log-name: <string>
      content-type: [blob|url]
....

.Example
[source,yaml]
....
capabilities:
  disable-guest-logs:
    # It's supported by driver, but maintainers do not wish to let users access live console of any guest from this pool.
    - log-name: console
      content-type: url

    # Also, don't expose /var/log/messages - driver calls this log `messages`, and
    # it's available only as a saved blob of text.
    - log-name: messages
      content-type: blob
....

=== Routing

==== Use pool only when requested explicitly by name

Supported by: `aws` `azure` `beaker` `localhost` `openstack`

Pools can be marked as available only when requested by name, via `environment.pool` field of the request. Such a pool would be ignored by the routing when processing requests that did not request the particular pool, making it effectively invisible for more relaxed requests.

.Specification
[source,yaml]
....
use-only-when-addressed: <boolean>  # default: false
....

.Example
[source,yaml]
....
- name: foo
  driver: beaker
  parameters:
    # Pool "foo" is backed by a Beaker instance, and therefore usually takes longer to provision a machine. Let's
    # make it available but only for users that are aware of this limitation, and ask for this pool directly.
    use-only-when-addressed: true
....
